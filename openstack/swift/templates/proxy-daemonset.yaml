{{- range $cluster_id, $cluster := .Values.clusters }}
{{- if $cluster.proxy_daemonset_enabled }}
kind: DaemonSet
apiVersion: apps/v1

metadata:
  name: swift-proxy-{{ $cluster_id }}
  labels:
    release: "{{ $.Release.Name }}"
    os-cluster: {{ $cluster_id }}

spec:
  minReadySeconds: 60 # longer wait here to be extra sure that the new servers are operational
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      component: swift-proxy-{{ $cluster_id }}
  template:
    metadata:
      labels:
        component: swift-proxy-{{ $cluster_id }}
        from: daemonset
        restart: carefully
      annotations:
        checksum/swift-cluster.etc: {{ include "swift/templates/etc/cluster-configmap.yaml" $ | sha256sum }}
        {{- include "swift_conf_annotations" $ | indent 8 }}
        checksum/swift.bin: {{ include "swift/templates/bin-configmap.yaml" $ | sha256sum }}
        {{- include "swift_ring_annotations" $ | indent 8 }}
        {{- if $.Values.health_exporter }}
        # Prometheus annotations for the swift-health-exporter, the annotations
        # for the statsd-exporter are on the proxy-service level instead
        prometheus.io/scrape: "true"
        prometheus.io/port: "9520"
        prometheus.io/targets: {{ required ".Values.alerts.prometheus.openstack missing" $.Values.alerts.prometheus.openstack }}
        {{- end }}
    spec:
      {{- include "swift_daemonset_tolerations" $ | indent 6 }}
      nodeSelector:
        species: {{ $.Values.species }}
      volumes:
        {{- tuple $cluster_id | include "swift_proxy_volumes" | indent 8 }}
      containers:
        {{- tuple "daemonset" $cluster $ | include "swift_proxy_containers" | indent 8 }}

---
{{- end }}
{{- end }}
